{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Augmentation : kogpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "#import os\n",
    "#os.chdir('./data')\n",
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dataset(dataset):\n",
    "    \"\"\"처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "    subject_entity = []\n",
    "    object_entity = []\n",
    "\n",
    "    for i, j in tqdm(zip(dataset[\"subject_entity\"], dataset[\"object_entity\"]), desc=\"preprocessing\"):\n",
    "        i = i[1:-1].split(\",\")[0].split(\":\")[1]\n",
    "        j = j[1:-1].split(\",\")[0].split(\":\")[1]\n",
    "\n",
    "        subject_entity.append(i)\n",
    "        object_entity.append(j)\n",
    "\n",
    "    out_dataset = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": dataset[\"id\"],\n",
    "            \"sentence\": dataset[\"sentence\"],\n",
    "            \"subject_entity\": subject_entity,\n",
    "            \"object_entity\": object_entity,\n",
    "            \"label\": dataset[\"label\"],\n",
    "        }\n",
    "    )\n",
    "    return out_dataset\n",
    "\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    \"\"\"csv 파일을 경로에 맡게 불러 옵니다.\"\"\"\n",
    "    pd_dataset = pd.read_csv(dataset_dir)\n",
    "    dataset = preprocessing_dataset(pd_dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def label_to_num(label):\n",
    "    num_label = []\n",
    "    with open(\"/opt/ml/code/dict_label_to_num.pkl\", \"rb\") as f:\n",
    "        dict_label_to_num = pickle.load(f)\n",
    "    for v in label:\n",
    "        num_label.append(dict_label_to_num[v])\n",
    "\n",
    "    return num_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **데이터 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preprocessing: 32470it [00:00, 354240.17it/s]\n"
     ]
    }
   ],
   "source": [
    "train_path = '/opt/ml/dataset/train/train.csv'\n",
    "total_data = load_data(train_path)\n",
    "# total_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = total_data.sample(frac=0.9, random_state=42)\n",
    "val_data = total_data.drop(train_data.index)\n",
    "\n",
    "train_label = label_to_num(train_data[\"label\"].values)\n",
    "val_label = label_to_num(val_data[\"label\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **생성 모델(koGPT-3)을 활용한 데이터 증강 실험**   \n",
    "- sub,obj entity 단어를 masking\n",
    "- 생성 모델로 entity 예측하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **model setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # fp16 버전의 리비전 모델 weight 파일을 제공\n",
    "  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  pad_token_id=tokenizer.eos_token_id,\n",
    "  torch_dtype='auto', low_cpu_mem_usage=True                # torch_dtype='auto' : fp16으로 자동으로 로드\n",
    ").to(device='cuda', non_blocking=True)                      # low_cpu_mem_usage=True > Huggingface의 Accelerator가 필요\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TEST01 : 실패**\n",
    "- 빈칸 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1년 후, 바이에른은 전설적인 [MASK]인 감독 에른스트 하펠이 이끄는 함부르크 SV와 DFB-포칼 결승에서 만났다. \n",
      "\n",
      "[MASK] :  DFB-포칼 결승에서 함부르크 SV에게 2-0으로 패했다. (출처: 'D\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "\n",
    "1년 후, 바이에른은 전설적인 [MASK]인 감독 에른스트 하펠이 이끄는 함부르크 SV와 DFB-포칼 결승에서 만났다. \n",
    "\n",
    "[MASK] : '''\n",
    "\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=60)\n",
    "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "  \n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TEST02 : X**\n",
    "- entity만 주어진 상태에서 새로운 문장 만들기\n",
    "- 오래 걸림, 매번 다른 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sentence :문하시중 이자연의 조카이고 인예왕후, 인경현비, 인절현비와는 사촌간이다.\n",
      "subject_entity : 인절현비\n",
      "object_entity : 이자연\n",
      "\n",
      "'인절현비'과 '이자연'의 관계를 설명하는 문장 :\n",
      "1) 인절현비 이자연의 고모이고, 인경현비 이자연의 고모이다.\n",
      "2)\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "\n",
    "sentence :문하시중 이자연의 조카이고 인예왕후, 인경현비, 인절현비와는 사촌간이다.\n",
    "subject_entity : 인절현비\n",
    "object_entity : 이자연\n",
    "\n",
    "'인절현비'과 '이자연'의 관계를 설명하는 문장 :\n",
    "'''\n",
    "\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.5, max_length=100)\n",
    "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "  \n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = '''\n",
    "# 1년 후, 바이에른은 전설적인 __인 감독 에른스트 하펠이 이끄는 함부르크 SV와 DFB-포칼 결승에서 만났다.\n",
    "\n",
    "# 문장 완성하기 :\n",
    "# '''\n",
    "# with torch.no_grad():\n",
    "#   tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "#   gen_tokens = model.generate(tokens, do_sample=True, temperature=0.85, max_length=64)     # temperature는 높을수록 모델이 다양한 결과값 도출. (반면, 0에 가까울 수록 거의 고정값과 같은 결과)\n",
    "#   generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "  \n",
    "# print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **KoGPT-3 연습하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 08:42:57.783944: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # fp16 버전의 리비전 모델 weight 파일을 제공\n",
    "  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 결과\n",
    "# tokenizer('점심메뉴 추천좀', '부대찌개 추천')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  pad_token_id=tokenizer.eos_token_id,\n",
    "  torch_dtype='auto', low_cpu_mem_usage=True                # torch_dtype='auto' : fp16으로 자동으로 로드\n",
    ").to(device='cuda', non_blocking=True)                      # low_cpu_mem_usage=True > Huggingface의 Accelerator가 필요\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TEST**\n",
    "- 소설쓰기\n",
    "- Q&A\n",
    "- 요약하기\n",
    "- 말투 번역\n",
    "- 영어 번역(temperature 낮게)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**소설쓰기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "제목 : 그녀는 예뻤다\n",
      "내용 : 그녀는 내 옆으로 살포시 다가와 내 볼을 어루만지기 시작했다. \n",
      "그리고, 그녀는 말했다. '조금만 더 가까이 와줘..' 나는 그대로 다가갈 수 밖에 없었다.\n",
      "나를 꼭 껴안아주는 그녀.\n",
      "그녀의 등뒤로 그녀의 향기가 느껴졌다.\n",
      "나는 더이상 아무 말도 하지 못 했다..\n",
      "그렇게 얼마의 시간이 지났을까..\n",
      "나는 그녀를 보았다. 그녀는... 눈물 범벅이 되어있었다.\n",
      "나는 그녀에게 달려가 그녀를 부축했다. 그리고, 나는 그녀를 내게 기대게 했다.\n",
      "그녀는 그 상태로 쓰러져버렸다.\n",
      "나는 그녀의 등을 두들겨 주었다. 그러자, 그녀는 나를 꽉 안아버렸다.\n",
      "나는 그대로 그녀의 등에 얼굴을 묻었다.\n",
      "내 등을 타고 그녀의 눈물이 흘러내렸다..\n",
      "나는 그대로 그녀의 눈물을 닦아주었다.\n",
      "그녀는 날 껴안고 계속 울었다.\n",
      "그렇게 얼마나 흘렀을까.. 나는 나도 모르게 잠이 들었던 것 같았다.\n",
      "일어나보니 그녀는 나를 안고 자고있었다.\n",
      "나는 그녀를 깨우지 않으려고 그녀의 얼굴을 손으로 가리고 그녀의 곁으로 다가갔다.\n",
      "그녀는 나를 빤히\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "제목 : 그녀는 예뻤다\n",
    "내용 : 그녀는 내 옆으로 살포시 다가와 내 볼을 어루만지기 시작했다. \n",
    "그리고, 그녀는 말했다. \\'조금만 더 가까이 와줘..\\' 나는 그대로 다가갈 수 밖에 없었다.\n",
    "'''\n",
    "\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.85, max_length=300)     # temperature는 높을수록 모델이 다양한 결과값 도출. (반면, 0에 가까울 수록 거의 고정값과 같은 결과)\n",
    "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "  \n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q&A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "최고 핫한 인공지능, kogpt님과 인터뷰 나눠보겠습니다!\n",
      "Q : kogpt님, 수월한 대화가 가능하신가요?\n",
      "A :  저는 사람을 많이 상대하는 일을 합니다. 제가 주로 하는 일은 영상을 찍고 편집하는 일이죠.\n",
      "Q : kogpt님께서는 어떻게 시작하셨나요?\n",
      "A : 블로그를 통해 영상을 만들고 유튜브에 업로드하고있었어요. 그러다 우연히 영상 크리에이터 모집 공지가 올라왔고 제가 좋아하는 유튜브를 하고 싶어 지원했어요.\n",
      "Q : 영상편집을 주로 하셔서 그런지 영상에 대한 감각이 좋으신거같아요.\n",
      "A : 네. 감각도 어느정도 있다고 생각해요.\n",
      "Q : 그럼 Kogpt님의 강점은 무어라고 생각하시나요?\n",
      "A : 저는 영상을 보는 사람이 제 영상을 계속 보고 싶은 힘을 가지는 게 저의 장점이라고 생각해요.\n",
      "Q : 제 영상을 보는 사람들이 계속 보고 싶어하게 하려면 어떤 점을 신경써야 하나요?\n",
      "A : 사실 처음 영상을 시작해서 잘하는 건 없어요. 하지만 제가 편집한 영상을 꾸준히 업로드하면서, 구독자가 조금씩 늘어나는 게 보이면 뿌듯함을 느껴요.\n",
      "Q : 구독자 수가 늘어나면 기쁘시겠어요.\n",
      "A : 확실히 구독자 수가 쌓이고 있다는 사실을 알게 된다면 정말 기뻐요. 저의 영상을 계속 볼만한 사람이 있구나 라는 거죠.\n",
      "Q : 영상을 업로드하실 때 어떤 부분에 중점을 두고 하나요?\n",
      "A : 사실 처음 영상을 시작하면서 어떤 콘텐츠를 만들어야 할지 고민이 많았어요. 그래서 생각한 건 내가 좋아하는 걸 계속해서 하자는 거였죠. 음악을 듣고, 게임을 하고, 책을 읽고, 영화를 보는 거에요. 제가 보고 싶은 영상을 제가 선택해서 만든 영상이에요.\n",
      "Q : 콘텐츠는 주로 어떤 것을 업로드하시나요?\n",
      "A : 네, 저는 주로 영화를 업로드해요. 하지만 영상을 업로드할 때는 제가 좋아하는 콘텐츠를 다루고 있어요.\n",
      "Q : 다른 사람과 다르게 콘텐츠를 제작하시는 거군요?\n",
      "A : 아니요. 저는 영상을 보는 사람은 어떤 콘텐츠를 봐야 할지 고민하지 않아도 된다고 생각해요. 영상을 보고 '아, 저 사람은 저런\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "최고 핫한 인공지능, kogpt님과 인터뷰 나눠보겠습니다!\n",
    "Q : kogpt님, 수월한 대화가 가능하신가요?\n",
    "A : '''\n",
    "\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.85, max_length=300)\n",
    "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "  \n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**요약하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "암호화폐 전문 미디어 데일리호들에 따르면, 비트코인 가격 예측 모델 'S2F 모델'을 고안한 유명 애널리스트 플랜비(PlanB)가 최근 한 유튜브 채널에 출연해 \"블랙스완(도저히 일어나지 않을 것 같은 일이 실제로 일어나는 현상)을 배제한다면 모든 지표들이 비트코인의 강세를 가리키고 있다. 강세론자들에게 지금의 가격대는 최고의 매수 기회\"라고 말했다. 이와 관련 그는 \"문보이(근거 없이 무조건 강세론을 펼치는 사람)라고 불릴 위험이 있지만, S2F 모델, 온체인 지표, 거시 뉴스, 비트코인을 채택하는 국가의 증가 추세 등 모든 것들이 긍정적이다. 비트코인의 본격 상승장을 알리는 신호로 선물 마켓의 프리미엄(선물과 현물 가격차)을 주시하고 있다\"고 설명했다. 코인마켓캡 기준 BTC는 현재 2.21% 오른 41,547.39 달러에 거래되고 있다.\n",
      "\n",
      "한줄 요약 : \n",
      "블룸버그통신이 비트코인에 투자하는 펀드매니저들의 숫자가 크게 늘어났으며, 지난 몇 년간 침체하던 시장에 다시 활기가 돌고 있다고 보도했다.\n",
      "\n",
      "암호화폐 전문 미디어 데일리호들에 따르면, 비트코인 가격 예측 모델인 S2F(Scalable tofu(Bitcoin) Model)을 고안한 유명 애널리스트 플랜비(PlanB)가 최근 한 유튜브 채널에 출연해 \"블랙스완(도저히 일어나지 않을 것 같은 일이 실제로 일어나는 현상)을 배제한다면 모든 지표들이 비트코인의 강세를 가리키고 있다. 강세론자들에게 지금의 가격대는 최고의 매수 기회\"라고 말했다. 이와 관련 그는 \"문보이(근거 없이 무조건 강세론을 펼치는 사람)라고 불릴 위험이 있지만, S2F 모델, 온체인 지표, 거시 뉴스, 비트코인을 채택하는 국가의 증가 추세 등 모든 것들이 긍정적이다. 비트코인의 본격 상승장을 알리는 신호로 선물 마켓의 프리미엄(선물과 현물 가격차)을 주시하고 있다\"고 설명했다.[EOS]\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "암호화폐 전문 미디어 데일리호들에 따르면, 비트코인 가격 예측 모델 'S2F 모델'을 고안한 유명 애널리스트 플랜비(PlanB)가 최근 한 유튜브 채널에 출연해 \"블랙스완(도저히 일어나지 않을 것 같은 일이 실제로 일어나는 현상)을 배제한다면 모든 지표들이 비트코인의 강세를 가리키고 있다. 강세론자들에게 지금의 가격대는 최고의 매수 기회\"라고 말했다. 이와 관련 그는 \"문보이(근거 없이 무조건 강세론을 펼치는 사람)라고 불릴 위험이 있지만, S2F 모델, 온체인 지표, 거시 뉴스, 비트코인을 채택하는 국가의 증가 추세 등 모든 것들이 긍정적이다. 비트코인의 본격 상승장을 알리는 신호로 선물 마켓의 프리미엄(선물과 현물 가격차)을 주시하고 있다\"고 설명했다. 코인마켓캡 기준 BTC는 현재 2.21% 오른 41,547.39 달러에 거래되고 있다.\n",
    "\n",
    "한줄 요약 : \n",
    "'''\n",
    "\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.85, max_length=512)\n",
    "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "  \n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('lv2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "460daaacedb28129c65ab758cd23a5a0183e369e52854b14e0ab0a12ea940b5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
