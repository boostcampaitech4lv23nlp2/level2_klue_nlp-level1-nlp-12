
![header](https://capsule-render.vercel.app/api?type=transparent&animation=scaleIn&section=header&text=NLP%20ET&fontSize=70&desc=âœ¨NLP_Team_12âœ¨%20&descAlignY=80)

# 1. í”„ë¡œì íŠ¸ ê°œìš”   

<aside>
ğŸ’¡ Competitions : [NLP] ë¬¸ì¥ ë‚´ ê°œì²´ê°„ ê´€ê³„ ì¶”ì¶œ   
ë¬¸ì¥ì˜ ë‹¨ì–´(Entitiy)ì— ëŒ€í•œ ì†ì„±ê´€ ê´€ê³„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°

</aside>

### TimeLine
![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/696155c5-3b40-4039-92d7-d17771eed133/Untitled.png)

### í˜‘ì—… ë°©ì‹
> **Notion**
> 
- Team Notionì— ì‹¤í—˜ ê²°ê³¼ ê¸°ë¡
- Kanban Boardë¡œ ë‹´ë‹¹ì ë° ì§„í–‰ ìƒí™© ê³µìœ    
![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/aee0d5f8-4b31-42a3-abc8-210e5367913f/Untitled.png)   

> **Git**
> 
- Git Commit Message Convention
    - featÂ : ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€
    - fixÂ : ë²„ê·¸ ìˆ˜ì •
    - docsÂ : ë¬¸ì„œ ìˆ˜ì •
    - styleÂ : ì½”ë“œ í¬ë§·íŒ…, ì„¸ë¯¸ì½œë¡  ëˆ„ë½, ì½”ë“œ ë³€ê²½ì´ ì—†ëŠ” ê²½ìš°
    - refactorÂ : ì½”ë“œ ë¦¬í™í† ë§
    - testÂ : í…ŒìŠ¤íŠ¸ ì½”ë“œ, ë¦¬í™í† ë§ í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¶”ê°€
    - choreÂ : ë¹Œë“œ ì—…ë¬´ ìˆ˜ì •, íŒ¨í‚¤ì§€ ë§¤ë‹ˆì € ìˆ˜ì •
    - <ì°¸ê³ > ğŸ”— [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0-beta.2/#specification)
- Git flow
    - master : ì œí’ˆìœ¼ë¡œ ì¶œì‹œë  ìˆ˜ ìˆëŠ” ë¸Œëœì¹˜
    - develop : ë‹¤ìŒ ì¶œì‹œ ë²„ì „ì„ ê°œë°œí•˜ëŠ” ë¸Œëœì¹˜
    - feature : ê¸°ëŠ¥ì„ ê°œë°œí•˜ëŠ” ë¸Œëœì¹˜
- Pre-commit
    - CI/CD - black, isort, autoflake â†’ flake8
- Git hub action
    - Pre-commit : flake8
    - Commit Convention
   
# 2. í”„ë¡œì íŠ¸ íŒ€ êµ¬ì„± ë° ì—­í•    
ğŸ”¬**EDA** : ìš©ì°¬

> Exploratory Data Analysis, Reference searching
>

ğŸ—‚ï¸ **Data** : ê±´ìš°, ë‹¨ìµ

> Data Experiment, searching the pre-trained models
>

ğŸ§¬ **MODEL** : ì¬ë•, ì„í¬

> to reconstruct the baseline, searching the pre-trained models
>

# 3. í”„ë¡œì íŠ¸ ìˆ˜í–‰ ì ˆì°¨ ë° ë°©ë²•
## 1) EDA

### a. ë¬¸ì œ ì •ì˜

- Entityì˜ ìœ„ì¹˜ê°€ embedding sizeì¸ 512ë¥¼ ë„˜ì–´ê°€ëŠ” ë°ì´í„° í™•ì¸
    - Embedding sizeë¥¼ ë„˜ì–´ê°€ëŠ” ë°ì´í„° drop
- í•œìê°€ í¬í•¨ë˜ì–´ ìˆëŠ” ë°ì´í„° í™•ì¸
    - hanja ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•œ í•œì-í•œêµ­ì–´ ë³€í™˜
- Baseline preprocessing í•¨ìˆ˜ ì˜¤ë¥˜ í™•ì¸
- Entity typeë³„ í¸í–¥ í™•ì¸Â¹â¾
    - Entity typeë³„ class restriction ì‹¤í—˜

### b. ë°ì´í„° ì‹œê°í™”

- Sentence ë¬¸ì¥ ê¸¸ì´ ì‹œê°í™”
    - Train, test data ë¶„í¬ê°€ ë™ì¼í•œ ê²ƒ í™•ì¸
- Label ë¶„í¬ ì‹œê°í™”
- Source column ì‹œê°í™”
    - Wikipedia, wikitree, policy_briefing ë¶„í¬ í™•ì¸
    - ë¬¸ì–´ì²´ ë°ì´í„°ë¡œ pre-trainingí•œ ëª¨ë¸ì— ì§‘ì¤‘

### c. An Improved Baseline for Sentence-level Relation ExtractionÂ²â¾

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/23747f42-8c04-4c5b-b833-0798a327b0a5/Untitled.png)

ìœ„ ë‹¤ì„¯ ê°€ì§€ ì‹¤í—˜ ëª¨ë‘ ì‹¤í–‰í•œ ê²°ê³¼ ì„±ëŠ¥ ê°œì„ ì„ ì´ë£¨ì–´ë‚´ì§€ëŠ” ëª»í–ˆìŠµë‹ˆë‹¤.

## 2) Model

### a. Pytorch Lightning refactoring

- Pytorch Lightning ì´ì‹ ë° ì‹¤í—˜ ì§€ì›

### b. Training Config

- Optimizer
    - AdamW
    - weight decay : Overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì¶”ê°€
    - LR Scheduler
        - constant_warmup : ì •í•´ì§„ stepê¹Œì§€ LRì´ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ë©° ì´í›„ ê³ ì •ëœ ê°’ìœ¼ë¡œ í•™ìŠµ
        - cosine_warmup : warm up ê³¼ì • ì´í›„ cosine í•¨ìˆ˜ë¥¼ í†µí•´ LR scaling ìˆ˜í–‰
    - LR finder : Pytorch Lightiningì˜ lr_finder ê¸°ëŠ¥ì„ í†µí•´ ì´ˆê¸° LR ì„¤ì •Â³â¾

![Learning rate finder](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/53364874-b9c6-453e-9282-de81eb61c0c1/Untitled.png)

Learning rate finder

- Loss Function
    - CrossEntropy : (baseline loss) input logitsê³¼ target ì‚¬ì´ cross entropy loss ê³„ì‚°
    - Focal Loss : class imbalance datasetì—ì„œ class ë³„ ê°€ì¤‘ì¹˜ë¥¼ lossì— ë°˜ì˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©â´â¾
    - Label Smoothing Loss : hard targetì„ soft targetìœ¼ë¡œ ë°”ê¾¸ì–´ ëª¨ë¸ì˜ over confidenceë¬¸ì œë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ì‚¬ìš©
    - F1 Loss : classification loss ê³„ì‚°í•˜ê¸° ìœ„í•´ ì‚¬ìš©
- seed_everything & deterministicì„ ì‚¬ìš©í•˜ì—¬ ì¬í˜„ ë³´ì¥
- Mixed precision : GPU resourceë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë©° ì—°ì‚°ì†ë„ ì¦ê°€
- Stratified KFold : ë¶ˆê· í˜•í•œ ë°ì´í„°ì…‹ì´ê¸°ì— ëª¨ë¸ ì„±ëŠ¥ ì¼ë°˜í™”ë¥¼ ìœ„í•´ ì‚¬ìš©
- Batch Size Finder : largest batch sizeë¥¼ ì°¾ëŠ” ê¸°ëŠ¥

![cosine warmup](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4729728f-303c-44d4-ab52-52a22719eee2/Untitled.png)

cosine warmup

- ì‹¤í—˜ ê²°ê³¼, LR Scheduler ì¤‘ **cosine warmup** ì„ ì‚¬ìš©í–ˆì„ ë•Œ ì„±ëŠ¥í–¥ìƒì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì´ëŠ” schdulerê°€ ì´ˆê¸°ì— ì›Œë°ì—… ê¸°ê°„ì„ ê°€ì§ìœ¼ë¡œì¨, ëª¨ë¸ì´ early over-fittingë˜ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

### c. ë…¼ë¬¸ êµ¬í˜„

- R-Roberta : R-bert ë§¤ì»¤ë‹ˆì¦˜ì„ Robertaì— ì ìš©âµâ¾
- CoRE: Counterfactual Analysis based Relation Extractionâ¶â¾

## 3) ****Data Experiments****

### a. Data Augmentation

- **Data pre-processing :** EDAë¥¼ í†µí•´ í•œì, ì˜ë¬¸ì ë“±ì˜ ë¬¸ì ë¹„ìœ¨ì´ ë§ì€ ê²ƒì„ í™•ì¸í–ˆê³ , í•œìë¥¼ (í•œì)ë¼ëŠ” í˜•íƒœë¡œ ì „ì²˜ë¦¬í•˜ì—¬ ë°ì´í„° ì„±ëŠ¥ì„ ë¹„êµí•˜ì˜€ìŠµë‹ˆë‹¤. í•œì ì „ì²˜ë¦¬í•œ ëª¨ë¸(roberta-base_hanja_16_1e-05) : test_auprc 0.465 í–¥ìƒë˜ì—ˆìœ¼ë‚˜ test_accuracy,test_f1ëŠ” ì›ë³¸ ë°ì´í„° ëª¨ë¸ì´ ë” ë†’ì€ ì„±ëŠ¥ì„ ê°–ê³  ìˆì–´ ìœ„ ì‹¤í—˜ì€ ì œì™¸í–ˆìŠµë‹ˆë‹¤.
- **Back Translation :** í•œ-ì˜-í•œ ì—­ë²ˆì—­ì„ ì‹œë„í•˜ë ¤ê³  í•˜ì˜€ìœ¼ë‚˜, EDAë¥¼ í†µí•´ ëŒ€ë¶€ë¶„ì˜ ì˜ë¬¸ìë¥¼ í¬í•¨í•œ ë‹¨ì–´ëŠ” ê³ ìœ ëª…ì‚¬ë¡œ í™•ì¸ë˜ì–´ í•œ-ì¼-í•œ ì—­ë²ˆì—­ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ì—­ë²ˆì—­ ë°ì´í„° 20,011 rowsë¥¼ ì¶”ê°€í•˜ì˜€ì§€ë§Œ, ì›ë³¸ ë°ì´í„°ì˜ ì„±ëŠ¥ì´ ë” ë†’ê²Œ ë‚˜ì™€ ìœ„ ì‹¤í—˜ì„ ì œì™¸í–ˆìŠµë‹ˆë‹¤.
- **EDA(easy data augmentation)**ë¥¼ ì‹œë„í•˜ë ¤ê³  í•˜ì˜€ìœ¼ë‚˜, ì´ì „ ê¸°ìˆ˜ì—ì„œ ì‹¤íŒ¨í•œ ì‹¤í—˜ìœ¼ë¡œ í™•ì¸ë˜ì–´ ì œì™¸í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, SR(ë™ì˜ì–´ êµì²´)ë¥¼ ì‹œë„í•˜ì˜€ìœ¼ë‚˜ êµì²´í•  entity ë‹¨ì–´ëŠ” ì§€ëª…, ì´ë¦„, ê³ ìœ ëª…ì‚¬ ë‹¨ì–´ê°€ ë§ì•„ì„œ ìœ ì˜ì–´ êµì²´í•  ë°ì´í„°ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ í¸ìœ¼ë¡œ í™•ì¸ë˜ì–´ ì œì™¸í–ˆìŠµë‹ˆë‹¤.
- **Generation Model** : ìƒì„± ëª¨ë¸(koGPT3)ì„ ì‚¬ìš©í•˜ì—¬, sub-obj entityë¥¼ í™œìš©í•œ ìƒˆë¡œìš´ ë¬¸ì¥ì„ êµ¬í˜„í•˜ëŠ” í•˜ëŠ” í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‘ entityê´€ê³„ë¥¼ ì œëŒ€ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ì¥ì„ ì œëŒ€ë¡œ êµ¬í˜„í•˜ì§€ ëª»í•´ ìœ„ ì‹¤í—˜ì€ ì œì™¸í–ˆìŠµë‹ˆë‹¤.
- **Masked Language Model** : ë¬¸ì¥ì— sub,obj entity ë¶€ë¶„ì„ [MASK]ë¡œ ì²˜ë¦¬í•˜ê³  bert ëª¨ë¸ë¡œ [MASK]ì˜ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì°¾ëŠ” ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ entityì™€ ë‹¤ë¥¸ ë‹¨ì–´ë¥¼ ìƒì„±í•˜ê²Œ í•˜ì—¬ ìƒˆë¡œìš´ entityë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. ì¦ê°• ë°ì´í„° 58,446 rowsë¥¼ ì¶”ê°€í•œ ì‹¤í—˜ ê²°ê³¼, ì›ë³¸ ë°ì´í„°ë³´ë‹¤ í–¥ìƒëœ ì„±ëŠ¥ì„ í™•ì¸í•˜ì§€ ëª»í•˜ì—¬ ìœ„ ì‹¤í—˜ì€ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.

### b. ë…¼ë¬¸ êµ¬í˜„

- **Unipelt, Lora** : finetuning ì‹œ ì „ì²´ parmeter ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ì¶”ê°€ì ìœ¼ë¡œ í•™ìŠµê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë¸ì— ì¶”ê°€í•˜ì—¬ í•™ìŠµì˜ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ì‹¤í—˜ì„ ì§„í–‰í•˜ê³ ì í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ëª¨ë¸ ìˆ˜ì •ì— ì–´ë ¤ì›€ì´ ìˆì–´ êµ¬í˜„ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

## 4) Optimization

### a. ELECTRA, RoBERTa, R-RoBERTa, BigBird ë“± ë‹¤ì–‘í•œ PLMì— ëŒ€í•œ ìµœì í™” ìˆ˜í–‰

- **batch size**:16
**loss**: CrossEntropy loss, Label smoothing
**learning rate scheduler**: cosine warmup scheduler, constant warmup scheduler
**Initial learning rate**: LR finder ê¸°ë°˜ ëª¨ë¸ë³„ ìµœì  learning rate ì‚¬ìš© (ex. roberta-large: 2.12e-05)

## 5) Ensemble

### a. Soft Voting

- ëª¨ë¸ë³„ micro-f1 scoreë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¤‘ í‰ê· ì„ êµ¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
- ELECTRA(3ê°œ), RoBERTa(4ê°œ), R-RoBERTa(2ê°œ)ë¥¼ ì•™ìƒë¸”í•œ ê²°ê³¼, micro-f1 74.1204, auprc 77.1611 ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ìŠµë‹ˆë‹¤.

### b. Hard Voting

- ì˜ˆì¸¡ëœ ë¼ë²¨ê°’ì¸ pred_labelì„ ê¸°ì¤€ìœ¼ë¡œ ìµœë¹ˆê°’ì„ ë„ì¶œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.
- Soft Votingëœ probsë¥¼ ìœ ì§€í•˜ê³ , ì•™ìƒë¸”ëœ ELECTRA, RoBERTa, R-RoBERTa, BigBirdë¥¼ ì•™ìƒë¸”í•œ ê²°ê³¼, micro-f1 74.4339, auprc 77.1611ë¡œ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ìŠµë‹ˆë‹¤.

# 4. í”„ë¡œì íŠ¸ ìˆ˜í–‰ ê²°ê³¼

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/05467360-1efe-4365-88af-3417a6a31906/Untitled.png)

- ìµœì¢… ê²°ê³¼ 14 ìœ„ (14/14), F1 ê¸°ì¤€ 71.97 íšë“

# 5. ê²°ë¡ 

### PLì‚¬ìš©, ë…¼ë¬¸ ì°¸ê³  ë“±ì„ í†µí•´ ë‹¤ë°©ë©´ìœ¼ë¡œ ì½”ë“œ ê°œì„ ì„ ìˆ˜í–‰í–ˆìœ¼ë©°, ë°ì´í„° ë¶„ì„ ë° ì‹¤í—˜, ì•™ìƒë¸” ê³¼ì •ì„ í†µí•´ ìµœì¢… ê²°ê³¼ë¥¼ ë„ì¶œ

- ë°ì´í„° ë¶„ì„ì„ í†µí•œ ë°ì´í„° í’ˆì§ˆ ê°œì„ (data cleaning, data augmentation)
- ë°ì´í„°ì…‹ì— ì í•œí•œ PLM ì„ ì • ë° ìµœì í™”
- ë² ì´ìŠ¤ë¼ì¸ ì½”ë“œ ê°œì„ 
    - PL ì´ì‹ ë° ë‹¤ì–‘í•œ ê¸°ëŠ¥ ì¶”ê°€
- ë…¼ë¬¸ êµ¬í˜„ì„ í†µí•œ ì¶”ê°€ì ì¸ ëª¨ë¸ ê°œì„ ì„ ìˆ˜í–‰
    - R-roberta, CoRE, Unipelt ë“±ì˜ ë…¼ë¬¸ì„ ì°¸ê³  ë° êµ¬í˜„ì„ ìœ„í•œ ì‹¤í—˜ ìˆ˜í–‰
- ë‹¤ì–‘í•œ ê²°ê³¼ì— ëŒ€í•œ ì•™ìƒë¸”(Hard Voting)ì„ ìˆ˜í–‰

# 6. Future Study

### ì¶”ê°€ì ì¸ ê°œì„  ë°©í–¥

- ë°ì´í„° ë¶ˆê· í˜•ì— ë”°ë¥¸ ê°œì„  ë°©ë²• í•„ìš”
    - focal loss ë“±ì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë‚˜, ê°œì„ ë˜ì§€ ì•ŠìŒ
    - under/over samplingì— ëŒ€í•œ ì¶”ê°€ ì‹¤í—˜ í•„ìš”
- Promptë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„° inputì„ ë³€ê²½í•˜ëŠ” ì‹¤í—˜ í•„ìš”â·â¾
    - Baseline: sub_entity [SEP] obj_entity [SEP] sentence
    Prompt: [CLS] sub_entityì™€ obj_entityì˜ ê´€ê³„ëŠ”? [SEP] sentence

# 7. Appendix

- RoBERTa-Large Loss function ì„ íƒ

| Model | Loss function | Learning Rate | Epoch | Batch Size | F1 score | AUPRC | inference / F1 | inference / AUPRC |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| RoBERTa-Large | CrossEntropy | 2.1216154368926846e-05 | 2 | 16 | 84.009 | 80.142 | 69.66 | 74.39 |  |
|  | F1 loss | 2.1216154368926846e-05 |  | 16 | 60.035 | 26.119 |  |  |  |
|  | Label smoothing | 2.1216154368926846e-05 | 1 | 16 | 84.131 | 78.105 | 68.0612 | 70.4386 |  |
|  | Focal | 2.1216154368926846e-05 | 2 | 16 | 83.403 | 77.717 |  |  |  |
|  | CrossEntropy | 2.1216154368926846e-05 |  | 16 | 84.178 | 80.415 |  |  | scheduler |
|  | CrossEntropy | 2.1216154368926846e-05 | ê° 2 epoch | 16 |  |  | 66.0331 | 69.9273 | k-fold(5) |
|  | CrossEntropy  | 2.1216154368926846e-05 |  | 16 | 84.783 | 80.62 |  |  | scheduler -cosine warmup |
|  | CrossEntropy | 2.1216154368926846e-05 | 2 | 16 | 90.85 | 91.093 | 61.2271 | 63.5467 | augmentation  entities |

| Model | Loss function | Learning Rate | Scheduler | Batch Size | Data | F1 score | AUPRC |
| --- | --- | --- | --- | --- | --- | --- | --- |
| electra | CrossEntropy | 4.487453899331321e-05 |  | 16 | Original | 81.077 | 74.163 |
|  | Label smoothing | 4.487453899331321e-05 |  | 16 | Original | 80.552 | 71.444 |
|  | CrossEntropy | 4.487453899331321e-05 |  | 16 | Multi label Augmentation | 80.248 | 65.376 |
|  | Label smoothing | 4.487453899331321e-05 |  | 16 | Multi label Augmentation | 80.003 | 67.314 |
|  | CrossEntropy | 4.487453899331321e-05 | cosine_warmup | 16 | Original | 81.213 | 72.594 |
|  | Label smoothing | 4.487453899331321e-05 | cosine_warmup | 16 | Original | 80.418 | 70.923 |
|  | CrossEntropy | 4.487453899331321e-05 | cosine_warmup | 16 | Multi label Augmentation | 80.003 | 67.314 |
|  | Label smoothing | 4.487453899331321e-05 | cosine_warmup | 16 | Multi label Augmentation | 79.838 | 66.963 |
|  | Label smoothing |  | cosine_warmup |  | Original | 84.735 | 81.839 |
|  | CrossEntropy |  | cosine_warmup |  | Multi label Augmentation | 85.2134 | 80.60365 |
| RoBERTa-Large | Label smoothing |  | constant_warmup |  | Multi label Augmentation | 83.61 | 80.239 |
|  | Label smoothing |  | cosine_warmup |  | Multi label Augmentation | 83.61 | 80.239 |
| koBigbird | CrossEntropy |  | cosine_warmup |  | Original | 83.456 | 77.203 |
|  | Label smoothing |  | cosine_warmup |  | Original | 83.144 | 75.198 |
| RoBERTa-Large | CrossEntropy |  | cosine_warmup |  | Multi label Augmentation | 83.913 | 80.28 |
| RoBERTa-Large
(1 epoch) | CrossEntropy |  | cosine_warmup |  | Multi label Augmentation | 82.828 | 75.403 |
| koBigbird | CrossEntropy |  |  |  | Multi label Augmentation |  |  |
|  | Label smoothing |  |  |  | Multi label Augmentation |  |  |
|  | Focal |  |  |  | Multi label Augmentation |  |  |
| R-ROBERTa-Large | CrossEntropy | 1.3182567385564076e-05 | cosine_warmup | 16 | Multi label Augmentation | 83.913 | 80.28 |
| R-ROBERTa-Large | CrossEntropy | 1.3182567385564076e-05 | cosine_warmup | 16 | Original | 84.491 | 80.874 |

---

1. [Relation Classification with Entity Type Restriction](https://arxiv.org/pdf/2105.08393.pdf) 
2. [An Improved Baseline for Sentence-level Relation Extraction](https://arxiv.org/pdf/2102.01373.pdf)
3. [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/pdf/1506.01186.pdf)
4. [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002v2) - â€œFocal Loss **focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training.â€
5. [Enriching Pre-trained Language Model with Entity Information for Relation Classification](https://arxiv.org/abs/1905.08284) - R-RoBERTa êµ¬í˜„ ì°¸ê³ 
6. [Should We Rely on Entity Mentions for Relation Extraction? Debiasing Relation Extraction with Counterfactual Analysis](https://arxiv.org/pdf/2205.03784.pdf) - â€œCoREÂ (Counterfactual Analysis based Relation Extraction) debiasing method that guides the RE models to focus on the main effects ofÂ textual contextÂ without losing the entity informationâ€
7. [PTR: Prompt Tuning with Rules for Text Classification](https://arxiv.org/pdf/2105.11259.pdf)
